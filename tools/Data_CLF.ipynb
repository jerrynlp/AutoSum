{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#read Embedding if the word is in word_dict\n",
    "def read_embedding(word_dict, embedding_file_path):\n",
    "    embedding_file = open(embedding_file_path, 'rb')\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embedding_size))\n",
    "    for line in embedding_file:\n",
    "        terms = line.rstrip().split(' ')\n",
    "        if not len(terms) == embedding_size + 1:\n",
    "            continue\n",
    "        if terms[0] in word_dict:\n",
    "            ids = word_dict[terms[0]]\n",
    "            embedding_vec = np.asarray(terms[1:], dtype='float32')\n",
    "            embedding_matrix[ids] = embedding_vec\n",
    "    return embedding_matrix\n",
    "#transfer each word to word id\n",
    "def transfer_data(word_vec, word_dict):\n",
    "    vec = []\n",
    "    for word in word_vec:\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "        vec.append(word_dict[word])\n",
    "    return vec\n",
    "def sim_max(sentence, labelId, embedding_matrix):\n",
    "    max_sim = 0.0\n",
    "    for ids in sentence:\n",
    "        embedding = embedding_matrix[ids]\n",
    "        simlarity = 1.0 - cosine(embedding, embedding_matrix[labelId])\n",
    "        if max_sim < simlarity:\n",
    "            max_sim = simlarity\n",
    "    return max_sim\n",
    "def avg_embedding(sentence, embedding_matrix):\n",
    "    word_embeddings = []\n",
    "    for ids in sentence:\n",
    "        embedding = embedding_matrix[ids]\n",
    "        word_embeddings.append(embedding)\n",
    "    return np.mean(word_embeddings, axis = 0)\n",
    "\n",
    "#select sentences\n",
    "def filter_dataset_seq(labelId, sentences, embedding_matrix):\n",
    "    x = []\n",
    "    max_score = 0\n",
    "    max_sentence = []\n",
    "    for sentence in sentences:\n",
    "        cur_score = sim_max(sentence, labelId, embedding_matrix)\n",
    "        if cur_score > max_score:\n",
    "            max_score = cur_score\n",
    "            max_sentence = sentence\n",
    "    return avg_embedding(sentences, embedding_matrix), max_sentence, embedding_matrix[labelId]\n",
    "\n",
    "###################################################################\n",
    "# Read tag file\n",
    "###################################################################\n",
    "TAG_FILE_PATH = \"../../tag.list\"\n",
    "tag_map = {}\n",
    "tag_file = open(TAG_FILE_PATH, 'rb')\n",
    "for line in tag_file:\n",
    "    tag = line.rstrip()\n",
    "    add = True\n",
    "    for item in tag_map.keys():\n",
    "        if item == tag: #replacy by similarity() function later\n",
    "            add = False\n",
    "            break\n",
    "    if add:\n",
    "        tag_map[tag] = 0\n",
    "tag_file.close()\n",
    "###################################################################\n",
    "# Read label file\n",
    "# Positive Sample if Tag in\n",
    "# Nagetive Sample if Tag not in (Randomly picked up for balancing)\n",
    "###################################################################\n",
    "LABEL_FILE_PATH = \"../../0.part.tokens.label\"\n",
    "Label_file = open(LABEL_FILE_PATH, 'rb')\n",
    "sample_map = {}\n",
    "for line in Label_file:\n",
    "    terms = line.split('\\t')\n",
    "    if len(terms) <= 2:\n",
    "        continue\n",
    "    key = terms[0] + ' ' + terms[1]\n",
    "    local_map = {}\n",
    "    #positive\n",
    "    for term in terms[2:]:\n",
    "        words = term.split(' ')\n",
    "        if words[0] == 'not' or words[0] == 'no':\n",
    "            continue\n",
    "        if words[len(words) - 1] in tag_map:\n",
    "            local_map[words[len(words) - 1]] = 1\n",
    "    if len(local_map) == 0:\n",
    "        continue\n",
    "    #negative\n",
    "    positive_count = len(local_map)\n",
    "    for count in range(positive_count):\n",
    "        pos = random.randrange(0, len(tag_map))\n",
    "        while tag_map.keys()[pos] in local_map:\n",
    "            pos = random.randrange(0, len(tag_map))\n",
    "        local_map[tag_map.keys()[pos]] = 0\n",
    "    #record\n",
    "    sample_map[key] = []\n",
    "    for tag in local_map.keys():\n",
    "        sample_map[key].append([tag, local_map[tag]])\n",
    "Label_file.close()\n",
    "count = 0\n",
    "for sample in sample_map.values():\n",
    "    count += len(sample)\n",
    "print count\n",
    "###################################################################\n",
    "# Read Sentences\n",
    "###################################################################\n",
    "SENENCE_FILE_PATH = \"../../0.part.tokens.sentence.samples\"\n",
    "sentence_file = open(SENENCE_FILE_PATH, 'rb')\n",
    "sentence_map = {}\n",
    "word_dict = {}\n",
    "for line in lines:\n",
    "    terms = line.rstrip().split(\"\\t\")\n",
    "    if len(terms) <= 2:\n",
    "        continue\n",
    "    key = terms[0] + ' ' + terms[1]\n",
    "    if not key in sample_map:\n",
    "        continue\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for term in terms[2:]:\n",
    "        if term == '&&':\n",
    "            if len(sentence) > 5 and len(sentence) < 40:\n",
    "                sentences.append(transfer_data(sentence, word_dict))\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(term)\n",
    "    sentence_map[key] = sentences\n",
    "sentence_file.close()\n",
    "###################################################################\n",
    "# Read embedding\n",
    "###################################################################\n",
    "EMBEDDING_FILE_PATH = \"\"\n",
    "embedding_matrix = read_embedding(word_dict, EMBEDDING_FILE_PATH)\n",
    "###################################################################\n",
    "# Construct features\n",
    "###################################################################\n",
    "X = []\n",
    "y = []\n",
    "for key in sentence_map.keys():\n",
    "    for sample in sample_map[key]:\n",
    "        if not sample[0] in word_dict:\n",
    "            continue\n",
    "        context,sentence,label_embedding = \\\n",
    "            filter_dataset_seq(word_dict[sample[0]], sentence_map[key], embedding_matrix)\n",
    "        X.append([context,sentence,label_embedding])\n",
    "        y.append(sample[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
